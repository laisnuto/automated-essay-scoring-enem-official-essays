{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Nota Esperada - Zero Shot JBSC\n",
        "\n",
        "Este notebook calcula as notas esperadas (expected grades) usando softmax e probabilidades para os modelos zero-shot JBSC.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === IMPORTS ===\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from tqdm.notebook import tqdm\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import ast\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "DRIVE_BASE_PATH = \"/content/drive/MyDrive/enem_tcc_resultados\"\n",
        "os.makedirs(DRIVE_BASE_PATH, exist_ok=True)\n",
        "print(f\"‚úì Google Drive montado. Resultados ser√£o salvos em: {DRIVE_BASE_PATH}\")\n",
        "\n",
        "SAVE_DIR = os.path.join(DRIVE_BASE_PATH, \"grade_expectation\", \"zero_shot_jbsc\")\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "print(f\"‚úì Diret√≥rio de resultados: {SAVE_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === CARREGAR DATASET ===\n",
        "print(\"Carregando o dataset...\")\n",
        "dataset = load_dataset(\"laisnuto/self-collected-ENEM-dataset\", split=\"train\")\n",
        "\n",
        "# Filtrar apenas os anos de teste usados no fine-tuning\n",
        "anos_teste = [2016, 2018, 2022, 2023]\n",
        "df_full = dataset.to_pandas()\n",
        "df_test = df_full[df_full[\"ano\"].isin(anos_teste)].reset_index(drop=True)\n",
        "dataset = dataset.filter(lambda x: x[\"ano\"] in anos_teste)\n",
        "\n",
        "print(f\"‚úì Dataset carregado: {len(df_test)} reda√ß√µes\")\n",
        "dataset.to_pandas().head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === CONFIGURA√á√ÉO DOS MODELOS ===\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"‚úÖ GPU dispon√≠vel: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è GPU n√£o dispon√≠vel, usando CPU\")\n",
        "\n",
        "competencias = [1, 2, 3, 4, 5]\n",
        "\n",
        "model_types = {\n",
        "    \"bert-base\": \"kamel-usp/jbcs2025_bert-base-multilingual-cased-encoder_classification-C{}-essay_only\",\n",
        "    \"bertugues\": \"kamel-usp/jbcs2025_BERTugues-base-portuguese-cased-encoder_classification-C{}-essay_only\",\n",
        "    \"bertimbau\": \"kamel-usp/jbcs2025_bertimbau_base-C{}\"\n",
        "}\n",
        "\n",
        "# Carregar modelos e tokenizers\n",
        "models = {}\n",
        "tokenizers = {}\n",
        "\n",
        "for model_key, model_path_template in model_types.items():\n",
        "    print(f\"\\nüì¶ Carregando modelos: {model_key}\")\n",
        "    models[model_key] = {}\n",
        "    tokenizers[model_key] = {}\n",
        "\n",
        "    for c in competencias:\n",
        "        comp_key = f\"C{c}\"\n",
        "        try:\n",
        "            model_name = model_path_template.format(c)\n",
        "            print(f\"‚û°Ô∏è {comp_key} | Modelo: {model_name}\")\n",
        "\n",
        "            if model_key == \"bertimbau\":\n",
        "                tokenizer = AutoTokenizer.from_pretrained(\"neuralmind/bert-base-portuguese-cased\")\n",
        "                tokenizer.model_max_length = 512\n",
        "            else:\n",
        "                tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "            model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "            model = model.eval()\n",
        "\n",
        "            if torch.cuda.is_available():\n",
        "                model = model.to(device)\n",
        "                print(f\"   ‚úì Modelo movido para GPU\")\n",
        "\n",
        "            models[model_key][comp_key] = model\n",
        "            tokenizers[model_key][comp_key] = tokenizer\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Erro ao carregar modelo {model_name}: {e}\")\n",
        "            models[model_key][comp_key] = None\n",
        "            tokenizers[model_key][comp_key] = None\n",
        "\n",
        "print(\"\\n‚úÖ Carregamento conclu√≠do!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === FUN√á√ÉO PARA CALCULAR NOTA ESPERADA ===\n",
        "def calcular_nota_esperada(model, tokenizer, text, device=\"cpu\"):\n",
        "    \"\"\"\n",
        "    Calcula a nota esperada usando softmax e probabilidades.\n",
        "    Retorna a nota esperada como float.\n",
        "    \"\"\"\n",
        "    # Tokenizar texto\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    \n",
        "    # Obter sa√≠da do modelo\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    \n",
        "    # Calcular probabilidades usando softmax\n",
        "    probabilities = F.softmax(outputs.logits, dim=-1)[0]\n",
        "    \n",
        "    # Calcular nota esperada: 0*P(0) + 40*P(40) + 80*P(80) + 120*P(120) + 160*P(160) + 200*P(200)\n",
        "    nota_esperada = (0 * probabilities[0] + \n",
        "                     40 * probabilities[1] + \n",
        "                     80 * probabilities[2] + \n",
        "                     120 * probabilities[3] + \n",
        "                     160 * probabilities[4] + \n",
        "                     200 * probabilities[5]).item()\n",
        "    \n",
        "    return nota_esperada\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === CALCULAR NOTAS ESPERADAS ===\n",
        "resultados_por_modelo = {}\n",
        "\n",
        "for model_key in model_types.keys():\n",
        "    print(f\"\\nüìä Calculando notas esperadas para: {model_key}\")\n",
        "    \n",
        "    csv_path = os.path.join(SAVE_DIR, f\"notas_esperadas_{model_key}_zero_shot_jbsc.csv\")\n",
        "    \n",
        "    # Verificar se j√° existe CSV\n",
        "    if os.path.exists(csv_path):\n",
        "        print(f\"‚úÖ Carregando notas esperadas existentes de {csv_path}\")\n",
        "        df_resultados = pd.read_csv(csv_path)\n",
        "        print(f\"   Carregadas {len(df_resultados)} notas esperadas\")\n",
        "        resultados_por_modelo[model_key] = df_resultados\n",
        "        continue\n",
        "    \n",
        "    # Calcular notas esperadas\n",
        "    textos = dataset[\"texto\"]\n",
        "    notas_esperadas = {f\"C{c}\": [] for c in competencias}\n",
        "    \n",
        "    for texto in tqdm(textos, desc=f\"Processando {model_key}\"):\n",
        "        for c in competencias:\n",
        "            comp_key = f\"C{c}\"\n",
        "            \n",
        "            if models[model_key][comp_key] is None:\n",
        "                notas_esperadas[comp_key].append(np.nan)\n",
        "                continue\n",
        "            \n",
        "            try:\n",
        "                nota = calcular_nota_esperada(\n",
        "                    models[model_key][comp_key],\n",
        "                    tokenizers[model_key][comp_key],\n",
        "                    texto,\n",
        "                    device\n",
        "                )\n",
        "                notas_esperadas[comp_key].append(nota)\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è Erro ao calcular nota para {comp_key}: {e}\")\n",
        "                notas_esperadas[comp_key].append(np.nan)\n",
        "    \n",
        "    # Criar DataFrame com resultados\n",
        "    df_resultados = dataset.to_pandas().copy()\n",
        "    for c in competencias:\n",
        "        comp_key = f\"C{c}\"\n",
        "        df_resultados[f\"nota_esperada_{comp_key}\"] = notas_esperadas[comp_key]\n",
        "    \n",
        "    # Salvar CSV\n",
        "    df_resultados.to_csv(csv_path, index=False)\n",
        "    print(f\"‚úÖ Notas esperadas salvas em: {csv_path}\")\n",
        "    \n",
        "    resultados_por_modelo[model_key] = df_resultados\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === PLOTAR GR√ÅFICOS ===\n",
        "for model_key, df_resultados in resultados_por_modelo.items():\n",
        "    print(f\"\\nüìà Gerando gr√°ficos para: {model_key}\")\n",
        "    \n",
        "    # Processar coluna 'notas' se existir\n",
        "    if \"notas\" in df_resultados.columns:\n",
        "        df_resultados[\"notas\"] = df_resultados[\"notas\"].apply(\n",
        "            lambda x: ast.literal_eval(x) if isinstance(x, str) else x\n",
        "        )\n",
        "    \n",
        "    r2_scores = {}\n",
        "    \n",
        "    # Criar figura com 5 subplots lado a lado\n",
        "    fig, axes = plt.subplots(1, 5, figsize=(30, 6))\n",
        "    fig.suptitle(f'Nota Real vs Nota Esperada por Compet√™ncia - Modelo {model_key} (Zero Shot JBSC)',\n",
        "                 fontsize=18, fontweight='bold', y=1.02)\n",
        "    \n",
        "    for idx, c in enumerate(competencias):\n",
        "        comp_key = f\"C{c}\"\n",
        "        ax = axes[idx]\n",
        "        \n",
        "        # Extrair notas reais e esperadas\n",
        "        if \"notas\" in df_resultados.columns:\n",
        "            y_real = df_resultados[\"notas\"].apply(lambda x: x[c-1] if isinstance(x, (list, tuple)) else np.nan)\n",
        "        else:\n",
        "            y_real = df_resultados[comp_key] if comp_key in df_resultados.columns else pd.Series(dtype=float)\n",
        "        \n",
        "        y_esperada = df_resultados[f\"nota_esperada_{comp_key}\"]\n",
        "        \n",
        "        # Remover NaN\n",
        "        pares = pd.DataFrame({\"r\": y_real, \"e\": y_esperada}).dropna()\n",
        "        y_real_clean = pares[\"r\"].astype(int).values\n",
        "        y_esperada_clean = pares[\"e\"].astype(float).values\n",
        "        \n",
        "        if len(y_real_clean) == 0:\n",
        "            ax.text(0.5, 0.5, 'Sem dados', ha='center', va='center', transform=ax.transAxes)\n",
        "            ax.set_title(f'{comp_key}', fontsize=14, fontweight='bold')\n",
        "            continue\n",
        "        \n",
        "        # Calcular regress√£o linear: Y = nota esperada, X = nota real\n",
        "        X = y_real_clean.reshape(-1, 1)  # X = nota real\n",
        "        y = y_esperada_clean  # Y = nota esperada\n",
        "        reg = LinearRegression()\n",
        "        reg.fit(X, y)\n",
        "        slope = reg.coef_[0]\n",
        "        intercept = reg.intercept_\n",
        "        \n",
        "        # Calcular R¬≤\n",
        "        r2 = reg.score(X, y)\n",
        "        r2_scores[comp_key] = r2\n",
        "        \n",
        "        # Gerar pontos para a linha de regress√£o\n",
        "        x_line = np.array([0, 200])\n",
        "        y_line = slope * x_line + intercept\n",
        "        \n",
        "        # Criar scatter plot (sem jitter)\n",
        "        ax.scatter(y_real_clean, y_esperada_clean, alpha=0.5, s=30, edgecolors='black', linewidths=0.3)\n",
        "        \n",
        "        # Adicionar linha de refer√™ncia (y = x)\n",
        "        ax.plot([0, 200], [0, 200], 'r--', linewidth=2, label='y = x', alpha=0.8)\n",
        "        \n",
        "        # Adicionar linha de regress√£o linear\n",
        "        ax.plot(x_line, y_line, 'b-', linewidth=2,\n",
        "                label=f'Regress√£o: y = {slope:.3f}x + {intercept:.2f}', alpha=0.8)\n",
        "        \n",
        "        # Configurar eixos (X = nota real, Y = nota esperada)\n",
        "        ax.set_xlabel('Nota Real', fontsize=12, fontweight='bold')\n",
        "        if idx == 0:\n",
        "            ax.set_ylabel('Nota Esperada', fontsize=12, fontweight='bold')\n",
        "        ax.set_title(f'{comp_key}\\nR¬≤ = {r2:.4f}',\n",
        "                     fontsize=13, fontweight='bold')\n",
        "        \n",
        "        # Configurar limites e ticks do eixo X (nota real: 0-200, de 20 em 20)\n",
        "        ax.set_xlim(-10, 210)\n",
        "        ax.set_xticks(range(0, 201, 20))\n",
        "        ax.set_xticklabels(range(0, 201, 20), rotation=45, ha='right', fontsize=9)\n",
        "        \n",
        "        # Configurar limites e ticks do eixo Y (nota esperada: 0-200, de 20 em 20)\n",
        "        ax.set_ylim(-10, 210)\n",
        "        ax.set_yticks(range(0, 201, 20))\n",
        "        ax.set_yticklabels(range(0, 201, 20), fontsize=9)\n",
        "        \n",
        "        # Grid\n",
        "        ax.grid(True, alpha=0.3, linestyle='--', linewidth=0.8)\n",
        "        \n",
        "        # Legenda\n",
        "        ax.legend(loc='upper left', fontsize=8, framealpha=0.9)\n",
        "        \n",
        "        # Ajustar layout\n",
        "        ax.set_aspect('equal', adjustable='box')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    \n",
        "    # Salvar gr√°fico\n",
        "    plot_path = os.path.join(SAVE_DIR, f\"grafico_nota_real_vs_esperada_{model_key}_zero_shot_jbsc.png\")\n",
        "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
        "    print(f\"‚úÖ Gr√°fico salvo em: {plot_path}\")\n",
        "    \n",
        "    # Mostrar gr√°fico\n",
        "    plt.show()\n",
        "    \n",
        "    # Imprimir resumo dos R¬≤\n",
        "    print(f\"\\nüìä Resumo das M√©tricas por Compet√™ncia - {model_key.upper()}:\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"{'Compet√™ncia':<12} {'R¬≤':<12}\")\n",
        "    print(\"-\" * 60)\n",
        "    for comp_key in competencias:\n",
        "        comp_key_str = f\"C{comp_key}\"\n",
        "        if comp_key_str in r2_scores:\n",
        "            print(f\"{comp_key_str:<12} {r2_scores[comp_key_str]:<12.4f}\")\n",
        "    print(\"-\" * 60)\n",
        "    if r2_scores:\n",
        "        print(f\"{'M√âDIO':<12} {np.mean(list(r2_scores.values())):<12.4f}\")\n",
        "    print(\"=\" * 60)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
